import cv2
import numpy as np
import mediapipe as mp
import tflite_runtime.interpreter as tflite # ë¼ì¦ˆë² ë¦¬íŒŒì´ì—ì„œëŠ” ì´ê±¸ ì‚¬ìš©!
import requests # HTTP ìš”ì²­ì„ ë³´ë‚´ê¸° ìœ„í•´ requests ëª¨ë“ˆ ì¶”ê°€
import time # ì§€ì—° ì‹œê°„ ê´€ë¦¬ë¥¼ ìœ„í•´ time ëª¨ë“ˆ ì¶”ê°€

# -----------------------------
# TFLite ëª¨ë¸ ë¡œë“œ
# -----------------------------
try:
    interpreter = tflite.Interpreter(model_path="hand_model_last_compatible.tflite")
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    print("âœ… TFLite ëª¨ë¸ ë¡œë“œ ì„±ê³µ.")
except Exception as e:
    print(f"âŒ TFLite ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit()

# -----------------------------
# í´ë˜ìŠ¤ ì´ë¦„ (í•™ìŠµëœ ëª¨ë¸ì˜ class_namesì™€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨)
# -----------------------------
# ì´ ë¶€ë¶„ì€ hand_model_last_compatible.tflite ëª¨ë¸ì´ í•™ìŠµëœ í´ë˜ìŠ¤ ìˆœì„œì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
# ë§Œì•½ ëª¨ë¸ì´ í•™ìŠµëœ í´ë˜ìŠ¤ ìˆœì„œê°€ ë‹¤ë¥´ë‹¤ë©´ ì´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
class_names = ["fan_off", "fan_on", "light_off", "light_on"] 
print(f"ğŸŒŸ ì¸ì‹í•  ì œìŠ¤ì²˜ í´ë˜ìŠ¤: {class_names}")

# -----------------------------
# Mediapipe hands ì´ˆê¸°í™”
# -----------------------------
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False, 
    max_num_hands=1, 
    min_detection_confidence=0.7 # ì´ ì‹ ë¢°ë„ë³´ë‹¤ ë‚®ìœ¼ë©´ ì œìŠ¤ì²˜ë¡œ ì¸ì‹ ì•ˆí•¨
)
mp_draw = mp.solutions.drawing_utils
print("âœ… MediaPipe Hands ì´ˆê¸°í™” ì™„ë£Œ.")

# -----------------------------
# ì›¹ìº  ì—´ê¸°
# -----------------------------
cap = cv2.VideoCapture(0) # 0ì€ ê¸°ë³¸ ì›¹ìº 
if not cap.isOpened():
    print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ê°€ ì—°ê²°ë˜ì–´ ìˆê³  ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš© ì¤‘ì´ ì•„ë‹Œì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit()
print("âœ… ì›¹ìº  ì—´ê¸° ì„±ê³µ.")

# -----------------------------
# GPIO ì„œë²„ë¡œ ëª…ë ¹ ì „ì†¡ í•¨ìˆ˜
# -----------------------------
def send_to_gpio_server(cmd):
    try:
        url = "http://localhost:5000/control" # gpio_server.pyì˜ /control ì—”ë“œí¬ì¸íŠ¸
        res = requests.post(url, json={"cmd": cmd})
        print(f"ğŸ“¡ ëª…ë ¹ ì „ì†¡ë¨: {cmd}, ì‘ë‹µ: {res.text}")
    except requests.exceptions.ConnectionError:
        print(f"âŒ GPIO ì„œë²„ ì—°ê²° ì‹¤íŒ¨. 'gpio_server.py'ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ GPIO ì„œë²„ ìš”ì²­ ì‹¤íŒ¨ ({cmd}):", e)

# -----------------------------
# ì œìŠ¤ì²˜ ìƒíƒœ ì¶”ì  ë³€ìˆ˜ (ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì „ì†¡ ë°©ì§€)
# -----------------------------
last_command_sent = None # ë§ˆì§€ë§‰ìœ¼ë¡œ ì „ì†¡í•œ ëª…ë ¹
command_debounce_time = 2 # ëª…ë ¹ ì¬ì „ì†¡ê¹Œì§€ì˜ ìµœì†Œ ì‹œê°„ (ì´ˆ)
last_command_timestamp = time.time() # ë§ˆì§€ë§‰ ëª…ë ¹ ì „ì†¡ ì‹œê°„

# -----------------------------
# ë©”ì¸ ë£¨í”„
# -----------------------------
frame_count = 0
current_label = "No hand detected" # í˜„ì¬ í™”ë©´ì— í‘œì‹œë  ë¼ë²¨
last_detected_gesture = None # ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ì§€ëœ ì œìŠ¤ì²˜ í´ë˜ìŠ¤ ì´ë¦„

print("ğŸš€ ì œìŠ¤ì²˜ ì¸ì‹ì„ ì‹œì‘í•©ë‹ˆë‹¤. 'q'ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”.")
while True:
    ret, frame = cap.read()
    if not ret:
        print("âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        break

    frame_count += 1
    
    # ì´ë¯¸ì§€ ë°˜ì „ (í•„ìš”í•œ ê²½ìš°, ì…€ì¹´ ëª¨ë“œì²˜ëŸ¼ ë³´ì´ê²Œ í•¨)
    # frame = cv2.flip(frame, 1) 

    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(img_rgb)

    detected_this_frame = False

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            h, w, _ = frame.shape
            x_min, y_min = w, h
            x_max, y_max = 0, 0

            for lm in hand_landmarks.landmark:
                x, y = int(lm.x * w), int(lm.y * h)
                x_min = min(x_min, x)
                y_min = min(y_min, y)
                x_max = max(x_max, x)
                y_max = max(y_max, y)

            # íŒ¨ë”© ì¶”ê°€
            pad = 20
            x_min = max(x_min - pad, 0)
            y_min = max(y_min - pad, 0)
            x_max = min(x_max + pad, w)
            y_max = min(y_max + pad, h)

            hand_img = frame[y_min:y_max, x_min:x_max]
            if hand_img.size == 0:
                continue

            # TFLite ì…ë ¥ì— ë§ê²Œ ì „ì²˜ë¦¬
            hand_input = cv2.resize(hand_img, (128, 128))
            hand_input = hand_input.astype(np.float32) / 255.0
            hand_input = np.expand_dims(hand_input, axis=0)

            # TFLite ì¶”ë¡ 
            interpreter.set_tensor(input_details[0]['index'], hand_input)
            interpreter.invoke()
            preds = interpreter.get_tensor(output_details[0]['index'])[0]

            class_idx = np.argmax(preds)
            confidence = preds[class_idx]

            # ì¼ì • ì‹ ë¢°ë„ ì´ìƒì¼ ë•Œë§Œ ì œìŠ¤ì²˜ ì¸ì‹ ë° ëª…ë ¹ ì „ì†¡
            if confidence > 0.7: 
                current_label = f"{class_names[class_idx]} ({confidence:.2f})"
                detected_this_frame = True
                last_detected_gesture = class_names[class_idx] # ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ì§€ëœ ì œìŠ¤ì²˜ ì—…ë°ì´íŠ¸

                # ëª…ë ¹ ì „ì†¡ ë¡œì§
                current_time = time.time()
                # ë§ˆì§€ë§‰ìœ¼ë¡œ ë³´ë‚¸ ëª…ë ¹ê³¼ ë‹¤ë¥¸ ëª…ë ¹ì´ ê°ì§€ë˜ì—ˆê±°ë‚˜,
                # ê°™ì€ ëª…ë ¹ì´ë¼ë„ ì¼ì • ì‹œê°„(debounce_time)ì´ ì§€ë‚¬ì„ ê²½ìš°ì—ë§Œ ì „ì†¡
                if (last_command_sent != last_detected_gesture) or \
                   (current_time - last_command_timestamp > command_debounce_time):
                    
                    if last_detected_gesture == "fan_on":
                        send_to_gpio_server("motor_on")
                        last_command_sent = "fan_on"
                        last_command_timestamp = current_time
                    elif last_detected_gesture == "fan_off":
                        send_to_gpio_server("motor_off")
                        last_command_sent = "fan_off"
                        last_command_timestamp = current_time
                    elif last_detected_gesture == "light_on":
                        send_to_gpio_server("light_on")
                        last_command_sent = "light_on"
                        last_command_timestamp = current_time
                    elif last_detected_gesture == "light_off":
                        send_to_gpio_server("light_off")
                        last_command_sent = "light_off"
                        last_command_timestamp = current_time
            else:
                # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ "No hand detected"ë¡œ í‘œì‹œí•˜ì§€ ì•Šê³ , ì´ì „ ë¼ë²¨ ìœ ì§€ (ì‹œê°ì  ì•ˆì •ì„±)
                pass # ì´ ê²½ìš° current_labelì€ ì´ì „ ê°’ ìœ ì§€

            # ê°ì§€ëœ ì† ë°”ìš´ë”© ë°•ìŠ¤ ë° ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
    
    # ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ì„ ë•Œ ë¼ë²¨ ì—…ë°ì´íŠ¸
    if not detected_this_frame:
        current_label = "No hand detected"
        # ì†ì´ ì—†ì–´ë„ last_command_sentë¥¼ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ (ì´ì „ ëª…ë ¹ ìœ ì§€)

    # í™”ë©´ì— í˜„ì¬ ë¼ë²¨ í‘œì‹œ
    cv2.putText(
        frame,
        current_label,
        (10, 30),
        cv2.FONT_HERSHEY_SIMPLEX,
        1,
        (0, 255, 255),  # ë…¸ë€ìƒ‰
        2,
    )

    cv2.imshow("Smart Hand Control (TFLite)", frame)
    if cv2.waitKey(1) & 0xFF == ord("q"):
        print("ğŸ‘‹ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ìš”ì²­.")
        break

# ì¢…ë£Œ ì‹œ ìì› í•´ì œ
cap.release()
cv2.destroyAllWindows()
print("Clean up: ì›¹ìº  ë° ì°½ í•´ì œ.")
