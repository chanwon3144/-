import cv2
import numpy as np
import threading
import time
import requests
from tensorflow.keras.models import load_model
from ultralytics import YOLO # ì´ ëª¨ë“ˆì€ hand_gesture_threadì—ì„œëŠ” ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, ì „ì²´ í”„ë¡œì íŠ¸ êµ¬ì„±ìƒ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.

# --- ë¶€ì € ì„¤ì • (gpiozero) ---
try:
    from gpiozero import TonalBuzzer
    # buzzer = TonalBuzzer(13) # ì‹¤ì œ ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ
    IS_RASPBERRY_PI = True
    print("ë¼ì¦ˆë² ë¦¬ íŒŒì´ í™˜ê²½ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ë¶€ì € ì‚¬ìš© ê°€ëŠ¥.")
except (ImportError, Exception):
    print("ì•Œë¦¼: ë¼ì¦ˆë² ë¦¬ íŒŒì´ í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. ë¶€ì € ëŒ€ì‹  ì½˜ì†”ì— ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.")
    IS_RASPBERRY_PI = False

# --- GPIO ë° ìŠ¤í”¼ì»¤ ì „ì†¡ í•¨ìˆ˜ ---
def send_to_gpio(cmd):
    try:
        # GPIO ì„œë²„ URLì€ í”„ë¡œì íŠ¸ ì„¤ì •ì— ë§ê²Œ ë³€ê²½ (localhost:5000 ë˜ëŠ” ë¼ì¦ˆë² ë¦¬íŒŒì´ IP:5000)
        gpio_server_url = "http://localhost:5000/control"
        res = requests.post(gpio_server_url, json={"cmd": cmd})
        print(f"ğŸ“¡ GPIO ì„œë²„ ì‘ë‹µ({cmd}):", res.text)
    except requests.exceptions.ConnectionError:
        print(f"âŒ GPIO ì„œë²„ ì—°ê²° ì‹¤íŒ¨. '{cmd}' ëª…ë ¹ ì „ì†¡ ë¶ˆê°€. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ GPIO ì „ì†¡ ì‹¤íŒ¨ ({cmd}):", e)

def send_to_speaker(cmd):
    try:
        # ìŠ¤í”¼ì»¤ ì„œë²„ URLì€ ì‹¤ì œ ìŠ¤í”¼ì»¤ ì„œë²„ì˜ IPì™€ í¬íŠ¸ì— ë§ê²Œ ë³€ê²½
        speaker_url = "http://10.10.15.167:8000/notify"
        res = requests.post(speaker_url, json={"cmd": cmd})
        print(f"ğŸ”Š ìŠ¤í”¼ì»¤ ì‘ë‹µ({cmd}):", res.text)
    except Exception as e:
        print(f"âŒ ìŠ¤í”¼ì»¤ ì „ì†¡ ì‹¤íŒ¨ ({cmd}):", e)

# --- ì† ì œìŠ¤ì²˜ ì¸ì‹ ìŠ¤ë ˆë“œ ---
def hand_gesture_thread():
    # ëª¨ë¸ ë¡œë“œ ë° í´ë˜ìŠ¤ ì´ë¦„ ì •ì˜
    try:
        model = load_model("hand_model.h5")
        print("âœ… hand_model.h5 ëª¨ë¸ ë¡œë“œ ì„±ê³µ.")
    except Exception as e:
        print(f"âŒ hand_model.h5 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. ê²½ë¡œ ë° íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸.")
        return

    class_names = ["fan_off", "fan_on", "light_off", "light_on"]

    # MediaPipe Hands ì´ˆê¸°í™”
    try:
        import mediapipe as mp
        mp_hands = mp.solutions.hands
        hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7, model_complexity=0)
        mp_draw = mp.solutions.drawing_utils
        print("âœ… MediaPipe Hands ì´ˆê¸°í™” ì„±ê³µ.")
    except ImportError:
        print("âŒ MediaPipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install mediapipe' ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return
    except Exception as e:
        print(f"âŒ MediaPipe Hands ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return

    # ì¹´ë©”ë¼ ìë™ ê°ì§€ ë° ì—´ê¸°
    cap = None
    for i in range(3): # 0, 1, 2ë²ˆ ì¹´ë©”ë¼ ì‹œë„
        test_cap = cv2.VideoCapture(i)
        if test_cap.isOpened():
            print(f"âœ… ì¹´ë©”ë¼ {i} ì‚¬ìš© ê°€ëŠ¥. ì—°ê²°í•©ë‹ˆë‹¤.")
            cap = test_cap
            break
        test_cap.release() # ì—´ë¦¬ì§€ ì•Šìœ¼ë©´ ë°”ë¡œ ë‹«ìŒ
    if cap is None:
        print("âŒ ì¹´ë©”ë¼ ì—´ê¸° ì‹¤íŒ¨! ì¹´ë©”ë¼ê°€ ì—°ê²°ë˜ì–´ ìˆê³  ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš© ì¤‘ì´ ì•„ë‹Œì§€ í™•ì¸í•˜ì„¸ìš”.")
        return

    # --- ì œìŠ¤ì²˜ ì¸ì‹ ì•ˆì •í™”ë¥¼ ìœ„í•œ ë³€ìˆ˜ ---
    last_recognized_command_sent = None # ë§ˆì§€ë§‰ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì „ì†¡ëœ ëª…ë ¹ì–´ (ì¤‘ë³µ ë°©ì§€ìš©)
    current_frame_detected_gesture = None # í˜„ì¬ í”„ë ˆì„ì—ì„œ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì œìŠ¤ì²˜ (ì‹ ë¢°ë„ ì¶©ì¡± ì‹œ)

    last_confirmed_gesture = None # ì—°ì†ì ìœ¼ë¡œ ì¸ì‹ë˜ì–´ 'í™•ì •'ëœ ì œìŠ¤ì²˜
    consecutive_frames_count = 0 # ë™ì¼ ì œìŠ¤ì²˜ ì—°ì† ì¸ì‹ í”„ë ˆì„ ì¹´ìš´íŠ¸
    required_consecutive_frames = 5 # ëª…ë ¹ ì „ì†¡ì„ ìœ„í•´ í•„ìš”í•œ ì—°ì† ì¸ì‹ í”„ë ˆì„ ìˆ˜ (ì¡°ì ˆ ê°€ëŠ¥)

    frame_counter = 0

    print("\n--- ì œìŠ¤ì²˜ ì¸ì‹ ì‹œì‘. 'q'ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œ ---")
    while True:
        ret, frame = cap.read()
        if not ret:
            print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨! ì¹´ë©”ë¼ ì—°ê²°ì´ ëŠì–´ì¡Œì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            break # í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨ ì‹œ ë£¨í”„ ì¢…ë£Œ

        frame_counter += 1
        # ë¼ì¦ˆë² ë¦¬íŒŒì´ CPU ê³¼ë¶€í•˜ ê°ì†Œë¥¼ ìœ„í•´ 3í”„ë ˆì„ ì¤‘ 1ë²ˆë§Œ ë¶„ì„
        if frame_counter % 3 != 0:
            cv2.imshow("Hand Gesture Camera (0)", frame)
            if cv2.waitKey(1) & 0xFF == ord("q"):
                break
            continue

        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(img_rgb)

        current_frame_detected_gesture = None # ë§¤ í”„ë ˆì„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                h, w, _ = frame.shape
                x_min, y_min = w, h
                x_max, y_max = 0, 0
                for lm in hand_landmarks.landmark:
                    x, y = int(lm.x * w), int(lm.y * h)
                    x_min = min(x_min, x)
                    y_min = min(y_min, y)
                    x_max = max(x_max, x)
                    y_max = max(y_max, y)

                # ì† ì˜ì—­ ì¶”ì¶œ ë° íŒ¨ë”© ì ìš©
                pad = 20
                x_min = max(x_min - pad, 0)
                y_min = max(y_min - pad, 0)
                x_max = min(x_max + pad, w)
                y_max = min(y_max + pad, h)
                hand_img = frame[y_min:y_max, x_min:x_max]

                if hand_img.size == 0 or hand_img.shape[0] < 10 or hand_img.shape[1] < 10:
                    continue # ìœ íš¨í•˜ì§€ ì•Šì€ ì† ì˜ì—­ì€ ê±´ë„ˆë›°ê¸°

                # ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                hand_input = cv2.resize(hand_img, (128, 128)) / 255.0
                hand_input = np.expand_dims(hand_input, axis=0)

                # ëª¨ë¸ ì˜ˆì¸¡
                preds = model.predict(hand_input, verbose=0)
                class_idx = np.argmax(preds[0])
                confidence = preds[0][class_idx]

                # ì‹ ë¢°ë„ 0.8 ì´ìƒì¼ ë•Œë§Œ ìœ íš¨í•œ ì œìŠ¤ì²˜ë¡œ ê°„ì£¼
                if confidence > 0.8:
                    current_frame_detected_gesture = class_names[class_idx]
                    label = f"{current_frame_detected_gesture} ({confidence:.2f})"
                    cv2.putText(frame, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)

                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        # --- ì œìŠ¤ì²˜ ì¸ì‹ í›„ ì²˜ë¦¬ ë¡œì§ (í•µì‹¬ ë³€ê²½ ë¶€ë¶„) ---
        if current_frame_detected_gesture is not None: # í˜„ì¬ í”„ë ˆì„ì—ì„œ ìœ íš¨í•œ ì œìŠ¤ì²˜ê°€ ì¸ì‹ëœ ê²½ìš°
            if current_frame_detected_gesture == last_confirmed_gesture:
                consecutive_frames_count += 1
            else:
                consecutive_frames_count = 1 # ìƒˆë¡œìš´ ì œìŠ¤ì²˜ê°€ ì¸ì‹ë˜ë©´ ì¹´ìš´íŠ¸ 1ë¶€í„° ì‹œì‘
                last_confirmed_gesture = current_frame_detected_gesture # í™•ì • ëŒ€ê¸° ì œìŠ¤ì²˜ ì—…ë°ì´íŠ¸

            # ì¶©ë¶„í•œ í”„ë ˆì„ ë™ì•ˆ ë™ì¼ ì œìŠ¤ì²˜ê°€ ì—°ì†ì ìœ¼ë¡œ ì¸ì‹ë˜ì—ˆê³ ,
            # ì´ì „ì— ë³´ë‚¸ ëª…ë ¹ê³¼ ë‹¤ë¥¸ ì œìŠ¤ì²˜ì¸ ê²½ìš°ì—ë§Œ ëª…ë ¹ ì „ì†¡
            if consecutive_frames_count >= required_consecutive_frames:
                if last_confirmed_gesture != last_recognized_command_sent:
                    cmd_to_send = ""
                    if last_confirmed_gesture == "fan_on":
                        cmd_to_send = "motor_on"
                    elif last_confirmed_gesture == "fan_off":
                        cmd_to_send = "motor_off"
                    elif last_confirmed_gesture == "light_on":
                        cmd_to_send = "light_on"
                    elif last_confirmed_gesture == "light_off":
                        cmd_to_send = "light_off"

                    if cmd_to_send:
                        send_to_gpio(cmd_to_send)
                        threading.Thread(target=send_to_speaker, args=(cmd_to_send,), daemon=True).start()
                        last_recognized_command_sent = last_confirmed_gesture # ëª…ë ¹ ì „ì†¡ëœ ì œìŠ¤ì²˜ë¡œ ì—…ë°ì´íŠ¸
                        print(f"âœ¨ ëª…ë ¹ ì „ì†¡! ì œìŠ¤ì²˜: '{last_confirmed_gesture}', ëª…ë ¹: '{cmd_to_send}'")
                        # ëª…ë ¹ ì „ì†¡ í›„, ë‹¤ìŒ ìƒˆë¡œìš´ ì œìŠ¤ì²˜ë¥¼ ë°›ê¸° ìœ„í•´ í™•ì • ìƒíƒœ ì´ˆê¸°í™” (ì„ íƒì )
                        # ì—¬ê¸°ì„œëŠ” ë™ì¼ ì œìŠ¤ì²˜ëŠ” í•œ ë²ˆë§Œ ë³´ë‚´ê³  ë‹¤ìŒ ë‹¤ë¥¸ ì œìŠ¤ì²˜ê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” ë¡œì§ì´ë¯€ë¡œ,
                        # last_recognized_command_sentë§Œ ì—…ë°ì´íŠ¸í•˜ê³  consecutive_frames_countëŠ” ìœ ì§€í•˜ì—¬
                        # ê°™ì€ ì œìŠ¤ì²˜ë¥¼ ê³„ì† í•´ë„ ë‹¤ì‹œ ë³´ë‚´ì§€ ì•Šë„ë¡ í•¨.
                # else:
                #     print(f"DEBUG: ë™ì¼ ì œìŠ¤ì²˜ '{last_confirmed_gesture}' ì—°ì† ì¸ì‹ ì¤‘, ì´ë¯¸ ëª…ë ¹ ì „ì†¡ë¨.")

        else: # í˜„ì¬ í”„ë ˆì„ì—ì„œ ìœ íš¨í•œ ì œìŠ¤ì²˜ê°€ ì¸ì‹ë˜ì§€ ì•Šì€ ê²½ìš° (ì†ì„ ì¹˜ì› ê±°ë‚˜, ì‹ ë¢°ë„ ë¯¸ë‹¬)
            # ì´ì „ì— ëª…ë ¹ì„ ë³´ë‚¸ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ë‹¤ìŒ ìƒˆë¡œìš´ ì œìŠ¤ì²˜ë¥¼ ë°›ì„ ì¤€ë¹„
            if last_recognized_command_sent is not None:
                print("--- ì œìŠ¤ì²˜ ì¸ì‹ ì—†ìŒ. ë‹¤ìŒ ëª…ë ¹ ëŒ€ê¸° ìƒíƒœë¡œ ì´ˆê¸°í™” ---")
            last_recognized_command_sent = None
            last_confirmed_gesture = None
            consecutive_frames_count = 0
        # --- ì œìŠ¤ì²˜ ì¸ì‹ í›„ ì²˜ë¦¬ ë¡œì§ ë ---

        cv2.imshow("Hand Gesture Camera (0)", frame)
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    # ìì› í•´ì œ
    cap.release()
    cv2.destroyAllWindows()
    print("ğŸ‘‹ ì œìŠ¤ì²˜ ì¸ì‹ ìŠ¤ë ˆë“œ ì¢…ë£Œ.")


# --- í”„ë¡œê·¸ë¨ ì‹œì‘ì  ---
if __name__ == "__main__":
    hand_gesture_thread()
