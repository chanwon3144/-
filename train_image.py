from google.colab import files
import zipfile
import os
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Conv2D,
    MaxPooling2D,
    Flatten,
    Dense,
    Dropout,
    BatchNormalization,
    Rescaling,
)
from tensorflow.keras.callbacks import EarlyStopping

# ------------------------------------------
# ğŸ“Œ Colabì—ì„œ ZIP ì—…ë¡œë“œ
# ------------------------------------------
uploaded = files.upload()

# ì—…ë¡œë“œëœ íŒŒì¼ëª… í™•ì¸ (í•˜ë‚˜ë§Œ ì—…ë¡œë“œí–ˆì„ ë•Œ)
for fname in uploaded.keys():
    zip_path = os.path.join("/content", fname)
    break

# ------------------------------------------
# ğŸ“Œ ì••ì¶• í’€ê¸°
# ------------------------------------------
extract_path = "/content/ai_project"  # zipì„ í’€ ê³³

with zipfile.ZipFile(zip_path, "r") as zip_ref:
    zip_ref.extractall(extract_path)

print("ì••ì¶• í•´ì œ ì™„ë£Œ:", os.listdir(extract_path))

# ğŸš€ ì‹¤ì œ í´ë˜ìŠ¤ í´ë”ê°€ ìˆëŠ” ê²½ë¡œë¡œ ì§€ì •
dataset_path = os.path.join(
    extract_path, "train_set"
)  # <= train_set ì•ˆì— í´ë˜ìŠ¤ í´ë”ê°€ ìˆìœ¼ë¯€ë¡œ

train_ds = tf.keras.utils.image_dataset_from_directory(
    dataset_path,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(128, 128),
    batch_size=32,
    label_mode="categorical",
)
val_ds = tf.keras.utils.image_dataset_from_directory(
    dataset_path,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(128, 128),
    batch_size=32,
    label_mode="categorical",
)


# ------------------------------------------
# ğŸ“Œ ì •ê·œí™”
# ------------------------------------------
normalization_layer = Rescaling(1.0 / 255)
train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))

# ------------------------------------------
# ğŸ“Œ CNN ëª¨ë¸
# ------------------------------------------
model = Sequential(
    [
        Conv2D(
            32, (3, 3), activation="relu", padding="same", input_shape=(128, 128, 3)
        ),
        BatchNormalization(),
        Conv2D(32, (3, 3), activation="relu", padding="same"),
        BatchNormalization(),
        MaxPooling2D(2, 2),
        Dropout(0.25),
        Conv2D(64, (3, 3), activation="relu", padding="same"),
        BatchNormalization(),
        Conv2D(64, (3, 3), activation="relu", padding="same"),
        BatchNormalization(),
        MaxPooling2D(2, 2),
        Dropout(0.25),
        Conv2D(128, (3, 3), activation="relu", padding="same"),
        BatchNormalization(),
        Conv2D(128, (3, 3), activation="relu", padding="same"),
        BatchNormalization(),
        MaxPooling2D(2, 2),
        Dropout(0.3),
        # ì¶”ê°€
        Conv2D(128, (3, 3), activation="relu", padding="same"),
        BatchNormalization(),
        MaxPooling2D(2, 2),
        Dropout(0.3),
        Flatten(),
        Dense(256, activation="relu"),
        BatchNormalization(),
        Dropout(0.5),
        Dense(128, activation="relu"),
        BatchNormalization(),
        Dropout(0.5),
        Dense(4, activation="softmax"),
    ]
)


model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# ------------------------------------------
# ğŸ“Œ í•™ìŠµ
# ------------------------------------------
early_stop = EarlyStopping(monitor="val_loss", patience=7, restore_best_weights=True)
model.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=[early_stop])

# ------------------------------------------
# ğŸ“Œ ì €ì¥
# ------------------------------------------
model.save("hand_model_split_auto2.h5")
