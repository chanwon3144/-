import cv2
import numpy as np
import threading
import time
import requests
from tensorflow.keras.models import load_model
from ultralytics import YOLO # ì´ ëª¨ë“ˆì€ hand_gesture_threadì—ì„œëŠ” ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, ì „ì²´ í”„ë¡œì íŠ¸ êµ¬ì„±ìƒ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.

# --- ë¶€ì € ì„¤ì • ---
try:
    from gpiozero import TonalBuzzer
    buzzer = TonalBuzzer(13)
    IS_RASPBERRY_PI = True
    print("ë¼ì¦ˆë² ë¦¬ íŒŒì´ í™˜ê²½ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. ë¶€ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
except (ImportError, Exception):
    print("ì•Œë¦¼: ë¼ì¦ˆë² ë¦¬ íŒŒì´ í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. ë¶€ì € ëŒ€ì‹  ì½˜ì†”ì— ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.")
    IS_RASPberry_PI = False

# --- GPIO ë° ìŠ¤í”¼ì»¤ ì „ì†¡ í•¨ìˆ˜ ---
def send_to_gpio(cmd):
    try:
        res = requests.post("http://localhost:5000/control", json={"cmd": cmd})
        print(f"ğŸ“¡ GPIO ì„œë²„ ì‘ë‹µ({cmd}):", res.text)
    except requests.exceptions.ConnectionError: # ì—°ê²° ì˜¤ë¥˜ë§Œ íŠ¹ì •í•˜ì—¬ ì¶œë ¥
        print(f"âŒ GPIO ì„œë²„ ì—°ê²° ì‹¤íŒ¨. '{cmd}' ëª…ë ¹ ì „ì†¡ ë¶ˆê°€.")
    except Exception as e:
        print(f"âŒ GPIO ì „ì†¡ ì‹¤íŒ¨ ({cmd}):", e)

def send_to_speaker(cmd):
    try:
        speaker_url = "http://10.10.15.167:8000/notify"
        res = requests.post(speaker_url, json={"cmd": cmd})
        print(f"ğŸ”Š ìŠ¤í”¼ì»¤ ì‘ë‹µ({cmd}):", res.text)
    except Exception as e:
        print(f"âŒ ìŠ¤í”¼ì»¤ ì „ì†¡ ì‹¤íŒ¨ ({cmd}):", e)

# --- ì† ì œìŠ¤ì²˜ ì¸ì‹ ìŠ¤ë ˆë“œ ---
def hand_gesture_thread():
    model = load_model("hand_model.h5")
    class_names = ["fan_off", "fan_on", "light_off", "light_on"]

    import mediapipe as mp
    mp_hands = mp.solutions.hands
    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7, model_complexity=0)
    mp_draw = mp.solutions.drawing_utils

    # ì¹´ë©”ë¼ ìë™ ê°ì§€
    cap = None
    for i in range(3): # 0, 1, 2ë²ˆ ì¹´ë©”ë¼ ì‹œë„
        test_cap = cv2.VideoCapture(i)
        if test_cap.isOpened():
            print(f"âœ… ì¹´ë©”ë¼ {i} ì‚¬ìš© ê°€ëŠ¥")
            cap = test_cap
            break
    if cap is None:
        print("âŒ ì¹´ë©”ë¼ ì—´ê¸° ì‹¤íŒ¨! ì—°ê²° ìƒíƒœ í™•ì¸")
        return

    # --- ë³€ê²½ëœ ë¶€ë¶„ ì‹œì‘ ---
    last_recognized_gesture = None # ë§ˆì§€ë§‰ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì¸ì‹ë˜ì–´ ëª…ë ¹ì„ ë³´ë‚¸ ì œìŠ¤ì²˜
    # command_debounce_time ë³€ìˆ˜ëŠ” ë” ì´ìƒ í•„ìš” ì—†ìŒ
    # --- ë³€ê²½ëœ ë¶€ë¶„ ë ---

    frame_counter = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
            continue

        frame_counter += 1
        # í™€ìˆ˜ í”„ë ˆì„ì—ì„œë§Œ ì²˜ë¦¬í•˜ë„ë¡ ìˆ˜ì • (CPU ê³¼ë¶€í•˜ ê°ì†Œ ëª©ì )
        if frame_counter % 3 != 0: # 3í”„ë ˆì„ ì¤‘ 1ë²ˆë§Œ ì²˜ë¦¬ (1/3 í”„ë ˆì„ë§Œ ì²˜ë¦¬)
            cv2.imshow("Hand Gesture Camera (0)", frame)
            if cv2.waitKey(1) & 0xFF == ord("q"):
                break
            continue

        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(img_rgb)

        current_gesture = None # í˜„ì¬ í”„ë ˆì„ì—ì„œ ì¸ì‹ëœ ì œìŠ¤ì²˜ (ì¼ë‹¨ ì—†ìŒìœ¼ë¡œ ì´ˆê¸°í™”)

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                h, w, _ = frame.shape
                x_min, y_min = w, h
                x_max, y_max = 0, 0
                for lm in hand_landmarks.landmark:
                    x, y = int(lm.x * w), int(lm.y * h)
                    x_min = min(x_min, x)
                    y_min = min(y_min, y)
                    x_max = max(x_max, x)
                    y_max = max(y_max, y)

                pad = 20
                x_min = max(x_min - pad, 0)
                y_min = max(y_min - pad, 0)
                x_max = min(x_max + pad, w)
                y_max = min(y_max + pad, h)
                hand_img = frame[y_min:y_max, x_min:x_max]
                if hand_img.size == 0:
                    continue

                hand_input = cv2.resize(hand_img, (128, 128)) / 255.0
                hand_input = np.expand_dims(hand_input, axis=0)

                preds = model.predict(hand_input, verbose=0)
                class_idx = np.argmax(preds[0])
                confidence = preds[0][class_idx]

                if confidence > 0.8:
                    current_gesture = class_names[class_idx] # í˜„ì¬ í”„ë ˆì„ì—ì„œ ì¸ì‹ëœ ì œìŠ¤ì²˜ ì €ì¥
                    label = f"{current_gesture} ({confidence:.2f})"
                    cv2.putText(frame, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)

                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        # --- ë³€ê²½ëœ í•µì‹¬ ë¡œì§ ---
        if current_gesture is not None: # í˜„ì¬ í”„ë ˆì„ì—ì„œ ìœ íš¨í•œ ì œìŠ¤ì²˜ê°€ ì¸ì‹ëœ ê²½ìš°
            if current_gesture != last_recognized_gesture: # ë§ˆì§€ë§‰ìœ¼ë¡œ ë³´ë‚¸ ëª…ë ¹ê³¼ ë‹¤ë¥¸ ì œìŠ¤ì²˜ì¸ ê²½ìš°
                cmd = ""
                if current_gesture == "fan_on":
                    cmd = "motor_on"
                elif current_gesture == "fan_off":
                    cmd = "motor_off"
                elif current_gesture == "light_on":
                    cmd = "light_on"
                elif current_gesture == "light_off":
                    cmd = "light_off"

                if cmd: # ëª…ë ¹ì–´ê°€ ìœ íš¨í•˜ë©´
                    send_to_gpio(cmd)
                    threading.Thread(target=send_to_speaker, args=(cmd,), daemon=True).start()
                    last_recognized_gesture = current_gesture # ë§ˆì§€ë§‰ìœ¼ë¡œ ë³´ë‚¸ ì œìŠ¤ì²˜ ì—…ë°ì´íŠ¸
                    print(f"ğŸ‰ ìƒˆë¡œìš´ ì œìŠ¤ì²˜ ì¸ì‹: {current_gesture}, ëª…ë ¹ ì „ì†¡: {cmd}")
            # else: # ë™ì¼í•œ ì œìŠ¤ì²˜ê°€ ì—°ì†ìœ¼ë¡œ ì¸ì‹ë˜ì—ˆì§€ë§Œ, ì´ë¯¸ í•´ë‹¹ ëª…ë ¹ì´ ì „ì†¡ëœ ìƒíƒœ
            #     print(f"DEBUG: ë™ì¼ ì œìŠ¤ì²˜ '{current_gesture}' ë°˜ë³µ ì¸ì‹ (ëª…ë ¹ ë¯¸ì „ì†¡)")
        else: # í˜„ì¬ í”„ë ˆì„ì—ì„œ ì•„ë¬´ ì œìŠ¤ì²˜ë„ ì¸ì‹ë˜ì§€ ì•Šê±°ë‚˜ ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²½ìš°
            if last_recognized_gesture is not None: # ì´ì „ì— ì¸ì‹ëœ ì œìŠ¤ì²˜ê°€ ìˆì—ˆë‹¤ë©´ ì´ˆê¸°í™”
                print("DEBUG: ì† ì œìŠ¤ì²˜ ì¸ì‹ ì—†ìŒ. ìƒíƒœ ì´ˆê¸°í™”.")
            last_recognized_gesture = None # ë‹¤ìŒ ì œìŠ¤ì²˜ë¥¼ ë°›ê¸° ìœ„í•´ ìƒíƒœ ì´ˆê¸°í™”
        # --- ë³€ê²½ëœ í•µì‹¬ ë¡œì§ ë ---

        cv2.imshow("Hand Gesture Camera (0)", frame)
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    cv2.destroyAllWindows()

# --- í”„ë¡œê·¸ë¨ ì‹œì‘ì  ---
if __name__ == "__main__":
    hand_gesture_thread()
