import threading
import queue
import time
import requests
from flask import Flask, request
from gtts import gTTS
import os
import logging

log = logging.getLogger("werkzeug")
log.setLevel(logging.ERROR)

# -----------------------------
# ğŸ”ˆ Flask ìŒì„± í”¼ë“œë°± ì„œë²„
# -----------------------------
app = Flask(__name__)


def speak(text):
    tts = gTTS(text=text, lang="ko")
    tts.save("speech.mp3")
    os.system("mpg123 -q speech.mp3")


@app.route("/notify", methods=["POST"])
def notify():
    data = request.get_json()
    cmd = data.get("cmd")
    if cmd == "light_on":
        speak("ì¡°ëª…ì„ ì¼­ë‹ˆë‹¤")
    elif cmd == "light_off":
        speak("ì¡°ëª…ì„ ë•ë‹ˆë‹¤")
    elif cmd == "motor_on":
        speak("ì„ í’ê¸°ë¥¼ ì¼­ë‹ˆë‹¤")
    elif cmd == "motor_off":
        speak("ì„ í’ê¸°ë¥¼ ë•ë‹ˆë‹¤")
    else:
        print("ğŸ”‡ ëª…ë ¹ ì—†ìŒ:", cmd)
    return "OK", 200


def run_flask_server():
    print("ğŸš€ Flask ìŒì„± ì„œë²„ ì‹œì‘ (port 8000)")
    app.run(host="0.0.0.0", port=8000, debug=False)


# -----------------------------
# ğŸ§  LLM & Queue
# -----------------------------
import json


def ask_ollama(prompt):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "gemma3:1b",
            "prompt": f"""
ë„ˆëŠ” smart homeì„ ì œì–´í•˜ëŠ” ëª¨ë¸ì´ì•¼ ë‹¤ìŒ ë¬¸ì¥ì„ ë°˜ë“œì‹œ ì•„ë˜ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë³€í™˜í•´. ê·¼ë° ë¬¸ì¥ì€ ì§ì ‘ ëª…ë ¹ì¼ ìˆ˜ë„ ìˆê³  ê°ì •/ìƒíƒœ í‘œí˜„ì¼ ìˆ˜ë„ ìˆì–´: light_on / light_off / motor_on / motor_off
ì˜ˆì‹œ:
- "ë¶ˆ ì¼œ" â†’ light_on
- "ì¡°ëª… ì¼œì¤˜" â†’ light_on
- "ë¶ˆ êº¼ì¤˜" â†’ light_off
- "ì¡°ëª… êº¼" â†’ light_off
- "light on" â†’ light_on
- "light off" â†’ light_off
- "ì„ í’ê¸° ì¼œì¤˜" â†’ motor_on
- "ì„ í’ê¸° êº¼ì¤˜" â†’ motor_off
- "fan on" â†’ motor_on
- "fan off" â†’ motor_off
- "ë¥ë‹¤" â†’ motor_on
- "ì¶¥ë‹¤" â†’ motor_off
- "ì–´ë‘¡ë‹¤" â†’ light_on
- "ë°ë‹¤" â†’ light_off
- "ìì•¼ê² ë‹¤" â†’ light_off
- "ë„ˆë¬´ ì¶”ì›Œ" â†’ motor_off
- "ë„ˆë¬´ ì¶¥ë‹¤" â†’ motor_off
- "ë„ˆë¬´ ë°ì•„" â†’ light_off
- "ë„ˆë¬´ ì–´ë‘ì›Œ" â†’ light_on
- "ë„ˆë¬´ ë”ì›Œ" â†’ motor_on
- "ë„ˆë¬´ ë¥ë‹¤" â†’ motor_on
ë¬¸ì¥: "{prompt}"
ì •ë‹µ:""",
            "stream": False
        }
    )
    return response.json()["response"].strip()


def send_worker():
    while True:
        cmd = cmd_queue.get()
        try:
            # Raspberry Pi ì „ì†¡
            res1 = requests.post("http://10.10.15.195:5000/control", json={"cmd": cmd})
            print("ğŸ“¡ Pi ì‘ë‹µ:", res1.text)
        except Exception as e:
            print("âŒ Pi ì „ì†¡ ì‹¤íŒ¨:", e)

        try:
            # ìŠ¤í”¼ì»¤ ì „ì†¡
            res2 = requests.post("http://localhost:8000/notify", json={"cmd": cmd})
            print("ğŸ”Š ìŠ¤í”¼ì»¤ ì‘ë‹µ:", res2.text)
        except Exception as e:
            print("âŒ ìŠ¤í”¼ì»¤ ì „ì†¡ ì‹¤íŒ¨:", e)
        cmd_queue.task_done()


# -----------------------------
# ğŸ§µ ë©”ì¸
# -----------------------------
if __name__ == "__main__":
    cmd_queue = queue.Queue()

    # Flask ì„œë²„ + Worker ìŠ¤ë ˆë“œ ì‹œì‘
    threading.Thread(target=run_flask_server, daemon=True).start()
    threading.Thread(target=send_worker, daemon=True).start()
    time.sleep(1)

    while True:
        print("", flush=True)  # ë²„í¼ ê¹¨ë—í•˜ê²Œ
        user_input = input("ëª…ë ¹ ì…ë ¥ (exit ì…ë ¥ ì‹œ ì¢…ë£Œ): ")
        if user_input.lower() == "exit":
            break

        cmd = ask_ollama(user_input)
        print("ğŸ“¥ LLM ì‘ë‹µ:", cmd)

        if cmd in ["light_on", "light_off", "motor_on", "motor_off"]:
            cmd_queue.put(cmd)
        else:
            print("âš ï¸ ì‹¤í–‰í•  ëª…ë ¹ì´ ì•„ë‹˜ (unknown)")
